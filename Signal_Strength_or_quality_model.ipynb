{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP7W1oMExn9ART89oXRFBKn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksekhar/Signal-Strength-or-quality-model./blob/master/Signal_Strength_or_quality_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hellping Class and functions"
      ],
      "metadata": {
        "id": "ccQCVKvIa3TY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79GCfspUDtZg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn import model_selection\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,BatchNormalization\n",
        "\n",
        "import random\n",
        "from tensorflow.keras import backend\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_drive(fpath):\n",
        "  try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive/\", force_remount=True)\n",
        "    google_drive_prefix = \"/content/drive/My Drive\"\n",
        "    data_path = f\"{google_drive_prefix}/{fpath}\"\n",
        "    print(data_path)\n",
        "    return data_path\n",
        "  except ModuleNotFoundError:\n",
        "    data_prefix = f\"{fpath}\""
      ],
      "metadata": {
        "id": "_nKlZ7QKugDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_name):\n",
        "  print(f\"reding the file: {file_name}\")\n",
        "  try:\n",
        "    file_type = file_name.split('.')[1]\n",
        "    print(file_type)\n",
        "    if file_type == 'csv':\n",
        "      df = pd.read_csv(signal_file_path)\n",
        "    else:\n",
        "      pass\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    print(\"data write\")\n",
        "    raise e"
      ],
      "metadata": {
        "id": "g5Io3uTZAZxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_sample_data(df,records):\n",
        "  if not(records):\n",
        "    records = 5\n",
        "  print(f\"verifying first {records} records and last {records} records\")\n",
        "  return pd.concat([df.head(records),df.tail(records)])"
      ],
      "metadata": {
        "id": "b1O9HLEpKQ9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_percentage_of_missing_values(df):\n",
        "  print(\"Checking for missing values and print percentage for each attribute\")\n",
        "  return (round(df.isnull().sum() / (df.isnull().count())*100))"
      ],
      "metadata": {
        "id": "pmojHZSPdCpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_distribution(data,variable_name):\n",
        "\n",
        "  target_variable = data[variable_name]\n",
        "  plt.hist(target_variable)\n",
        "  plt.xlabel(f\"{variable_name}\")\n",
        "  plt.ylabel(\"Frequency\")\n",
        "  plt.title(\"Distribution of Target Variable\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "t65dFX7wB4KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tring_accuracy(history,t_lebel,v_level,y_level):\n",
        "\n",
        "  plt.figure(figsize=(5, 3))\n",
        "  plt.plot(history.history['accuracy'], color='b', label=t_lebel)\n",
        "  plt.plot(history.history['val_accuracy'], color='r', label=v_level)\n",
        "  plt.title(f\"{t_lebel} and {v_level}\")\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(y_level)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "F3-e7sqzY9_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data import and Understanding"
      ],
      "metadata": {
        "id": "41abRphQas4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "signal_file_path = mount_drive(\"dataset/NN Project Data - Signal.csv\")"
      ],
      "metadata": {
        "id": "Ti7ADL-UvR3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***a. Read the ‘Signals.csv’ as DataFrame and import required libraries.***"
      ],
      "metadata": {
        "id": "9Y12eGp5KHIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "signalDf = read_file(signal_file_path)\n",
        "verify_sample_data(signalDf,3)"
      ],
      "metadata": {
        "id": "3p_PVrsTtUD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signalDf.info()"
      ],
      "metadata": {
        "id": "6GcXdUDeeJhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The dataset consists of **1599 entries** (non-null values).\n",
        "- There are **12 columns** in total, labeled as `Parameter 1` through `Parameter 11` and `Signal_Strength`.\n",
        "- All `Parameter` columns contain **floating-point values** (float64), which suggests they could be measurements or scores of some kind.\n",
        "- The `Signal_Strength` column contains **integer values** (int64), which could be a count, a rating, or some other discrete value.\n",
        "- There are **no missing values** in the dataset, as indicated by the 'non-null' count for each column.\n"
      ],
      "metadata": {
        "id": "3prK1H26hxh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Check for missing values and print percentage for each attribute.**"
      ],
      "metadata": {
        "id": "Ad5p4TAiATvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_percentage_of_missing_values(signalDf)"
      ],
      "metadata": {
        "id": "r569wav1AANB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Parameter 1**: There are **96** unique values.\n",
        "2. **Parameter 2**: There are **143** unique values.\n",
        "3. **Parameter 3**: There are **80** unique values.\n",
        "4. **Parameter 4**: There are **91** unique values.\n",
        "5. **Parameter 5**: There are **153** unique values.\n",
        "6. **Parameter 6**: There are **60** unique values.\n",
        "7. **Parameter 7**: There are **144** unique values.\n",
        "8. **Parameter 8**: There are **436** unique values.\n",
        "9. **Parameter 9**: There are **89** unique values.\n",
        "10. **Parameter 10**: There are **96** unique values.\n",
        "11. **Parameter 11**: There are **65** unique values.\n",
        "\n",
        "The signal strength target variables is **6** unique values.\n"
      ],
      "metadata": {
        "id": "xcvXucbqHgaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Check for presence of duplicate records in the dataset and impute with appropriate method**"
      ],
      "metadata": {
        "id": "qj_b4zs-EtRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "signalDf.nunique()"
      ],
      "metadata": {
        "id": "gVMkuqOUE14e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`d. Visualise distribution of the target variable`**"
      ],
      "metadata": {
        "id": "MNOFWIziBn1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "signalDf[\"Signal_Strength\"].describe().T"
      ],
      "metadata": {
        "id": "-j5FBrObASJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_distribution(signalDf,\"Signal_Strength\")"
      ],
      "metadata": {
        "id": "2oMy9HDAA3wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Mean**: The mean (average) value of the target variable is approximately **5.64**.\n",
        "- **Standard Deviation**: The standard deviation measures the spread or variability of the target variable. In this case, it's approximately **0.81**.\n",
        "- **Minimum**: The minimum value of the target variable is **3**.\n",
        "- **25th Percentile (Q1)**: This represents the value below which 25% of the data falls. It's **5** in this case.\n",
        "- **Median (50th Percentile)**: The median (also known as the 50th percentile) is the middle value of the dataset when sorted. It's **6** here.\n",
        "- **75th Percentile (Q3)**: This represents the value below which 75% of the data falls. It's also **6**.\n",
        "\n",
        "From this information, we can infer that the majority of the target variable values are around **5** or **6**, with some variability. The distribution appears to be relatively centered."
      ],
      "metadata": {
        "id": "RFyI5KrDEWO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e. Share insights from the initial data analysis**"
      ],
      "metadata": {
        "id": "l51s1WLyJJmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "signalDf.describe().T"
      ],
      "metadata": {
        "id": "Ik3NtDW0bOGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Parameter 1**: The data is centered around 8.32 with a spread of 1.74. The wide range suggests diverse data points.\n",
        "\n",
        "2. **Parameter 2**: The average value is 0.53, indicating that most data points are close to this value. However, the maximum value of 1.58 suggests the presence of higher values as well.\n",
        "\n",
        "3. **Parameter 3**: With a mean of 0.27 and a maximum value of 1, it appears that the data points are skewed towards the lower end.\n",
        "\n",
        "4. **Parameter 4**: The data has a fairly large spread (std: 1.41) around the mean (2.54), indicating variability in the data points.\n",
        "\n",
        "5. **Parameter 5**: The data is tightly clustered around the mean (0.087), as indicated by the small standard deviation (0.047).\n",
        "\n",
        "6. **Parameter 6**: The large standard deviation (10.46) relative to the mean (15.87) suggests a wide spread in the data.\n",
        "\n",
        "7. **Parameter 7**: The data points are spread out over a large range (from 6 to 289), indicating high variability.\n",
        "\n",
        "8. **Parameter 8**: The data is tightly clustered around the mean (0.997), suggesting low variability.\n",
        "\n",
        "9. **Parameter 9**: The data points are relatively close to the mean (3.31), indicating moderate variability.\n",
        "\n",
        "10. **Parameter 10**: The data has a moderate spread (std: 0.17) around the mean (0.658), indicating some variability.\n",
        "\n",
        "11. **Parameter 11**: The data points are fairly close to the mean (10.42), suggesting moderate variability.\n",
        "\n",
        "12. **Signal Strength**: The average signal strength is 5.64 with a moderate spread (std: 0.81), indicating that the signal strength varies moderately across observations."
      ],
      "metadata": {
        "id": "1T0OHsGmguRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data preprocessing"
      ],
      "metadata": {
        "id": "bX7YTza3JZsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a.Split the data into X & Y**\n",
        "\n"
      ],
      "metadata": {
        "id": "S7a-_LHZJc6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = signalDf.drop(['Signal_Strength'],axis=1)\n",
        "Y = signalDf[['Signal_Strength']]"
      ],
      "metadata": {
        "id": "NiOfsSasLm_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YC = to_categorical(Y)"
      ],
      "metadata": {
        "id": "HHrvQ1uvBzE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b.Split the data into train & test with 70:30 proportion.**"
      ],
      "metadata": {
        "id": "cJX8yfdxLpo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,YC, test_size = 0.3, random_state = 42,stratify = Y)"
      ],
      "metadata": {
        "id": "4Tu59joJKrSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of training samples:\", X_train.shape[0])\n",
        "print(\"Number of testing samples:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "HvnQszytNasj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Print shape of all the 4 variables and verify if train and test data is in sync**"
      ],
      "metadata": {
        "id": "0NDim9-mJodR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes of all 4 variables\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Verify if train and test data is in sync\n",
        "if X_train.shape[0] == y_train.shape[0] and X_test.shape[0] == y_test.shape[0]:\n",
        "  print(\"Train and test data are in sync\")\n",
        "else:\n",
        "  print(\"Train and test data are not in sync\")"
      ],
      "metadata": {
        "id": "9ecR2dGZM7mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D. Normalise the train and test data with appropriate method**"
      ],
      "metadata": {
        "id": "k46MOHXPJsBJ"
      }
    },
    {
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_normalized = scaler.transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GmbIfnz4QVZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_normalized.shape);\n",
        "print(X_test_normalized.shape);"
      ],
      "metadata": {
        "id": "v58OIFx51pdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E. Transform Labels into format acceptable by Neural Network**"
      ],
      "metadata": {
        "id": "FpPst-N-Jv9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-ImSDyjW4DCy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_8-fpPupJZWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Training & Evaluation using Neural Network"
      ],
      "metadata": {
        "id": "M0WF5x0H3PFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "np.random.seed(42)\n",
        "import random\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "cK8vYMj_HxH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Design a Neural Network to train a classifier**"
      ],
      "metadata": {
        "id": "40mWwcDE3TK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(11, activation='relu',input_dim = X_train_normalized.shape[1]))\n",
        "model.add(Dense(21, activation='relu'))\n",
        "model.add(Dense(9, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "T2Oedv_-36yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "LhritfakI7L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Train the classifier using previously designed Architecture**"
      ],
      "metadata": {
        "id": "kqxjo-se3Y9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=X_train_normalized, y=y_train, batch_size=20, epochs=50, validation_data=(X_test_normalized, y_test))"
      ],
      "metadata": {
        "id": "hCxxns3Q37oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Plot 2 separate visuals.**\n",
        "\n",
        "  *i. Training Loss and Validation Loss*"
      ],
      "metadata": {
        "id": "CkyXEINx3eIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(history.history['loss'], color='b', label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], color='r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DR6TeCOk38aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph shows training and validation loss over epochs. The training loss decreases sharply then plateaus, while the validation loss decreases more gradually and slightly increases towards the end, indicating possible overfitting. Strategies like early stopping, dropout, or regularization could be used to improve the model's performance on unseen data."
      ],
      "metadata": {
        "id": "ZJ_JiYqfSWbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " *ii. Training Accuracy and Validation Accuracy*"
      ],
      "metadata": {
        "id": "DQ7VSgmKRnJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(history.history['accuracy'], color='b', label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], color='r', label='Validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-qMGdjYs38Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph shows the training and validation accuracy of a model over 50 epochs. Both accuracies increase until the 10th epoch, then plateau. The consistently higher training accuracy may indicate overfitting."
      ],
      "metadata": {
        "id": "J1w-aTJhS8GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D. Design new architecture/update existing architecture in attempt to improve the performance of the model.**"
      ],
      "metadata": {
        "id": "49hsg5KK3uGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "np.random.seed(42)\n",
        "import random\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "hpH6KBug396b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = Sequential()\n",
        "model3.add(Dense(256,activation='relu',input_dim = X_train_normalized.shape[1]))\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Dense(128,activation='relu'))\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Dense(64,activation='relu'))\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Dense(32,activation='relu'))\n",
        "model3.add(Dense(9, activation = 'softmax'))"
      ],
      "metadata": {
        "id": "qtx7cfy_T9D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "metadata": {
        "id": "9szvam-XWWdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "model3.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "WQK7_9cfWjPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E. Plot visuals as in Q3.C and share insights about difference observed in both the models.**"
      ],
      "metadata": {
        "id": "qQ8qdpJ23yMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_3 = model3.fit(X_train_normalized,y_train,batch_size=64,epochs=50,verbose=1,validation_data=(X_test_normalized, y_test))"
      ],
      "metadata": {
        "id": "7NVDkvC8Wz2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tring_accuracy(history_3,'Training Loss','Validation loss','loss')\n",
        "plot_tring_accuracy(history_3,'Training accuracy','Validation accuracy','accuracy')"
      ],
      "metadata": {
        "id": "egmCiC6m3-jy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}